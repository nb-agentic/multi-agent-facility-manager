#!/usr/bin/env python3
"""
Test the optimized CrewAI setup
"""
import asyncio
import sys
sys.path.append('.')

from intellicenter.core.async_crew import llm_manager, OptimizedLLMManager

async def test_llm_manager():
    """Test the optimized LLM manager"""
    print("ğŸ§ª Testing Optimized LLM Manager...")
    
    try:
        # Test getting different LLMs
        hvac_llm = llm_manager.get_llm("hvac")
        print(f"âœ… HVAC LLM loaded: {hvac_llm.model}")
        
        security_llm = llm_manager.get_llm("security")
        print(f"âœ… Security LLM loaded: {security_llm.model}")
        
        # Test memory report
        report = llm_manager.get_memory_report()
        print(f"ğŸ“Š {report}")
        
        # Test simple LLM call - CrewAI LLM doesn't have direct invoke method
        # We'll test through the async crew instead
        print(f"ğŸ¤– HVAC LLM loaded successfully: {hvac_llm.model}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(test_llm_manager())
    if success:
        print("ğŸ‰ Optimized LLM Manager is working!")
    else:
        print("ğŸ’¥ Test failed!")